{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fccbcb3b-b815-471c-8e3c-0c9f5d1f439a",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "1. [Initial Setup](#first-bullet)<br>\n",
    "    * [1.1 Installing Required Libraries](#first-bullet-sub-1)<br>\n",
    "    * [1.2 Importing required libraries](#first-bullet-sub-2)<br>\n",
    "2. [Loading Data](#second-bullet)<br>\n",
    "3. [LLM Prompting](#third-bullet)<br>\n",
    "    * [3.1 Create model prompt function](#third-bullet-sub-1)<br>\n",
    "    * [3.2 Prompting LLM](#third-bullet-sub-2)<br>\n",
    "    * [3.3 Executing LLM response](#third-bullet-sub-3)<br>\n",
    "4. [Proofs using Z3](#fourth-bullet)<br>\n",
    "    * [4.1 Create necessary Z3 variables](#fourth-bullet-sub-1)<br>\n",
    "    * [4.2 Define constraints and Z3 solver](#fourth-bullet-sub-2)<br>\n",
    "5. [Saving to S3](#fifth-bullet)<br>\n",
    "    * [Saving LLM response](#fifth-bullet-sub-1)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaec67d-2667-4ad9-95ef-a4042a94ee8a",
   "metadata": {},
   "source": [
    "The Notebook contains code snippets that recreates a Data Science / Appled Science user actions trying to access documents on the Onehouse data hub. Current demostration uses S3 bucket but in actuality it can be any database, S3 or other similar locations. The user action would call an LLM model of choice provided by Amazon Bedrock service. The user can use PDF and docx files and pass the data within these files to the supported LLM models like Anthropics Claude Opus / Sonnet / Haiku along with customized queries. The received responses can then be stored locally or in user specific S3 buckets as required.\n",
    "\n",
    "### 1. Initial Setup <a class = \"anchor\" id = \"first-bullet\"></a>\n",
    "\n",
    "#### 1.1 Installing Required Libraries <a class = \"anchor\" id = \"first-bullet-sub-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afca64d-86bf-4074-8732-13f2921d80b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdfplumber\n",
    "!pip install z3-solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63824cc7-a9ba-4e35-bb28-abf5493bc93f",
   "metadata": {},
   "source": [
    "#### 1.2 Importing required libraries <a class = \"anchor\" id = \"first-bullet-sub-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaeea11-94b3-4355-8d7b-0f68f6522906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pdfplumber\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "import logging \n",
    "import io\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91437f48-3140-4ea5-a54f-4319e93ce7c3",
   "metadata": {},
   "source": [
    "### 2. Loading Data <a class = \"anchor\" id = \"second-bullet\"></a>\n",
    "\n",
    "\n",
    "Downloading the specific pdf file from the user's S3 bucket. The user can browse through their S3 folder using the Amazon S3 console on this account. Accessing objects from any other S3 bucket results in an error. Modify the code in the cell to reflect the correct file name and the correct S3 bucket that the file is contained in.(There are currently no pdf samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb47f09-02f0-4107-98da-3fda999217db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pdf file from s3 bucket\n",
    "session = boto3.Session()\n",
    "s3 = session.client(service_name='s3')\n",
    "\n",
    "s3.download_file('onehouse-project-ds-user-bucket',\n",
    "                 'home/David/demo_ppa.pdf', 'demo.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19030995-1b17-4b55-8069-02a3ef55e42c",
   "metadata": {},
   "source": [
    "The pdf file is then converted into a string and stored in a variable so that it can be passed in as a query to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da0d172-83fd-4ef7-814a-a6fc9deedfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the PDF file\n",
    "with pdfplumber.open('demo.pdf') as pdf:\n",
    "    # Initialize an empty string to store the text\n",
    "    fullText_pdf = ''\n",
    "\n",
    "    # Loop through each page in the PDF\n",
    "    for page in pdf.pages:\n",
    "        # Extract the text from the page\n",
    "        page_text = page.extract_text()\n",
    "\n",
    "        # Append the page text to the overall text\n",
    "        fullText_pdf += page_text + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6563260-bc9c-4a4f-98ce-c0466b8235cf",
   "metadata": {},
   "source": [
    "### 3. LLM Prompting <a class = \"anchor\" id = \"third-bullet\"></a>\n",
    "\n",
    "#### 3.1 Create model prompt function <a class = \"anchor\" id =\"third-bullet-sub-1\"></a>\n",
    "\n",
    "The function below sends the provided content as a query to the specific LLM model provided by the user. Currently the only supported models are Anthropics Claude Opus/Haiku/Sonnet. The response from the model is returned as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8693c6-0045-4514-b9f2-ae200569c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that sends the request to the Claude model you want to send to, \n",
    "# requires the user to pass in the \"content\" portion of the body of the request\n",
    "def prompt_model(model_type: str, content: str):\n",
    "    if model_type == \"CLAUDE_OPUS\":\n",
    "        model = \"anthropic.claude-3-opus-20240229-v1:0\" # Model ID does not work for some reason\n",
    "    elif model_type == \"CLAUDE_SONNET\":\n",
    "        model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "    elif model_type == \"CLAUDE_HAIKU\":\n",
    "        model = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "    else: \n",
    "        print(\"Model ID not recognized\")\n",
    "\n",
    "    # Create a Bedrock Runtime client in the AWS Region of your choice.\n",
    "    client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "    # Set the model ID, e.g., Claude 3 Haiku.\n",
    "    model_id = model\n",
    "\n",
    "    # Format the request payload using the model's native structure. \n",
    "    native_request = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 10000,\n",
    "        \"temperature\": 0.5,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": json.loads(content),\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Convert the native request to JSON.\n",
    "    request = json.dumps(native_request)\n",
    "\n",
    "    # Invoke the model with the request.\n",
    "    streaming_response = client.invoke_model_with_response_stream(\n",
    "        modelId=model_id, body=request\n",
    "    )\n",
    "    response_text =\"\"\n",
    "    # Extract and print the response text in real-time.\n",
    "    for event in streaming_response[\"body\"]:\n",
    "        chunk = json.loads(event[\"chunk\"][\"bytes\"])\n",
    "        if chunk[\"type\"] == \"content_block_delta\":\n",
    "            response_text+=chunk[\"delta\"].get(\"text\", \"\")\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32aeb16-dd0e-45c1-87c9-26406e0d7ef5",
   "metadata": {},
   "source": [
    "#### 3.2 Prompting LLM <a class = \"anchor\" id = \"third-bullet-sub-2\"></a>\n",
    "\n",
    "The function above can be called using an example similar to the one below. The content has to be provided in json format and the pdf document stored in the variable fullText_pdf is stored can be passed to the LLM through the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872b160-8ba2-4602-aba4-3533d38ebd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sending prompt to model\n",
    "prompt = \"Create two strings start_date and end_date by writing code in python which correspond to the start and end of the discount term in the format YYYY-MM-DD.\"\n",
    "response = prompt_model(\"CLAUDE_SONNET\", json.dumps([{\"type\": \"text\", \"text\":fullText_pdf},\n",
    "                                         {\"type\": \"text\", \"text\": prompt}]))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b721da-f553-4be3-9a02-2d9028c92711",
   "metadata": {},
   "source": [
    "#### 3.3 Executing LLM response <a class = \"anchor\" id = \"third-bullet-sub-3\"></a>\n",
    "\n",
    "The cell below runs the python code generated by the LLM. It also stores all the variables created in the code that has been executed in the namespace 'variables'. Sometimes the generated response highlights the python code by including thriple ticks, otherwise it produces only the code as a response. The cell below accounts for both possibilities. If the LLM is requested to create the data and the user plans on using the data on their own after the LLM generated response is executed, it is important that they specify the name of the variable the data should be stored in so that they can access it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dd2ff8-361d-45b7-a6a1-de0a4ee4331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the start and end of the code block\n",
    "start = response.find('```python')\n",
    "end = response.find('```', start + 9)\n",
    "\n",
    "if start == -1 or end == -1:\n",
    "    code = response\n",
    "    namespace = {}\n",
    "    # Run the codes\n",
    "    try:\n",
    "        exec(code, namespace)\n",
    "        # Find all the variables in the namespace\n",
    "        variables = {k: v for k, v in namespace.items() if not k.startswith('__')}\n",
    "        print(\"Code executed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error executing the code:\"+str(e))\n",
    "else:\n",
    "    # Extract the code from the string\n",
    "    code = response[start+9:end]\n",
    "    namespace = {}\n",
    "    # Run the codes\n",
    "    try:\n",
    "        exec(code, namespace)\n",
    "        # Find all the variables in the namespace\n",
    "        variables = {k: v for k, v in namespace.items() if not k.startswith('__')}\n",
    "        print(\"Code executed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error executing the code:\"+str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e731958a-5cf6-4b9e-ab0a-f11887618c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3aa9b-ec46-4fae-be36-7e7ddc6c1361",
   "metadata": {},
   "source": [
    "### 4. Proofs using Z3 <a class = \"anchor\" id = \"fourth-bullet\"></a>\n",
    "\n",
    "The Z3 library is a powerful theorem prover and constraint solver developed by Microsoft Research. It can be used to solve a wide range of problems, including program verification, software testing, and logical reasoning. In this demonstration we will be using Z3 to create a simple consistency check in order to prove that the discount term end date is later than the discount term start date.\n",
    "\n",
    "#### 4.1 Create necessary Z3 variables <a class = \"anchor\" id = \"fourth-bullet-sub-1\"></a>\n",
    "\n",
    "In order to use the z3 solver, we will be counting the number of days that have been passed since a reference date. The dates that were provided as strings need to be converted into Z3 integer variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec4af1d-b603-4034-bc0b-4cbe9092a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from z3 import *\n",
    "from datetime import datetime, date\n",
    "\n",
    "\n",
    "reference_date = date(1970, 1, 1)\n",
    "start_date = Int('start_date')\n",
    "end_date = Int('end_date')\n",
    "\n",
    "start_date_obj = datetime.strptime(variables['start_date'], '%Y-%m-%d').date()\n",
    "end_date_obj = datetime.strptime(variables['end_date'], '%Y-%m-%d').date()\n",
    "\n",
    "start_date_days = (start_date_obj - reference_date).days\n",
    "end_date_days = (end_date_obj - reference_date).days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc231f34-ae74-494e-8a2c-646e7fa9abea",
   "metadata": {},
   "source": [
    "#### 4.2 Define constraints and Z3 solver <a class = \"anchor\" id = \"fourth-bullet-sub-2\"></a>\n",
    "\n",
    "We define a constraint that the start date is less than the end date and add it to a Z3 Solver. You can add multiple constraints to a Solver and check if they are all true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce22f2-923b-4d9b-8af5-c84966a252c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = [start_date < end_date] \n",
    "\n",
    "solver = Solver() \n",
    "solver.add(constraints) \n",
    "if solver.check() == sat:\n",
    "    print(\"The discount term dates are in logical order.\") \n",
    "else:\n",
    "    print(\"The discount term dates are not in logical order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560302cd-4973-4a07-97df-122a3df5644f",
   "metadata": {},
   "source": [
    "### 5. Saving to S3 <a class = \"anchor\" id = \"fifth-bullet\"></a>\n",
    "\n",
    "#### 5.1 Saving LLM response <a class = \"anchor\" id = \"fifth-bullet-sub-1\"></a>\n",
    "\n",
    "If necessary the response from the LLM can be stored in a text file and then saved in the user specific S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab4b70c-f096-42ef-911c-3082bd5e53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save response to a text file\n",
    "textFile = open(\"response.txt\",\"w\")  # Change to 'a' if you do not want to overwrite\n",
    "textFile.write(response)\n",
    "textFile.close()\n",
    "# Upload response text file to S3 bucket\n",
    "s3 = boto3.resource('s3') \n",
    "s3.Bucket('onehouse-project-ds-user-bucket').upload_file(\"response.txt\",'home/David/llm_response.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
